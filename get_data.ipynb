{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNq4BLexCGE9he91THkxsCI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ReAlex1902/stock-trend-pred/blob/main/get_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9i4vd5YbkhEa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b50a8c4-6abb-41b0-e2d6-72013fdee383"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/81.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m61.4/81.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sgmllib3k (from feedparser)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6048 sha256=dc72882ba95f8a4012bad6d5bee10dbdd80bb96261ed409848225cd63bbf8965\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser\n",
            "Successfully installed feedparser-6.0.11 sgmllib3k-1.0.0\n",
            "Collecting dateparser\n",
            "  Downloading dateparser-1.2.0-py2.py3-none-any.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from dateparser) (2.8.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from dateparser) (2023.3.post1)\n",
            "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27 in /usr/local/lib/python3.10/dist-packages (from dateparser) (2023.6.3)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from dateparser) (5.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->dateparser) (1.16.0)\n",
            "Installing collected packages: dateparser\n",
            "Successfully installed dateparser-1.2.0\n",
            "Collecting trafilatura\n",
            "  Downloading trafilatura-1.6.3-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from trafilatura) (2023.11.17)\n",
            "Collecting courlan>=0.9.5 (from trafilatura)\n",
            "  Downloading courlan-0.9.5-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting htmldate>=1.6.0 (from trafilatura)\n",
            "  Downloading htmldate-1.6.0-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.7/40.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting justext>=3.0.0 (from trafilatura)\n",
            "  Downloading jusText-3.0.0-py2.py3-none-any.whl (837 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m837.8/837.8 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.9.3 in /usr/local/lib/python3.10/dist-packages (from trafilatura) (4.9.3)\n",
            "Requirement already satisfied: charset-normalizer>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from trafilatura) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from trafilatura) (2.0.7)\n",
            "Requirement already satisfied: langcodes>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from courlan>=0.9.5->trafilatura) (3.3.0)\n",
            "Collecting tld>=0.13 (from courlan>=0.9.5->trafilatura)\n",
            "  Downloading tld-0.13-py2.py3-none-any.whl (263 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.8/263.8 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: dateparser>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from htmldate>=1.6.0->trafilatura) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from htmldate>=1.6.0->trafilatura) (2.8.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from dateparser>=1.1.2->htmldate>=1.6.0->trafilatura) (2023.3.post1)\n",
            "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27 in /usr/local/lib/python3.10/dist-packages (from dateparser>=1.1.2->htmldate>=1.6.0->trafilatura) (2023.6.3)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from dateparser>=1.1.2->htmldate>=1.6.0->trafilatura) (5.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->htmldate>=1.6.0->trafilatura) (1.16.0)\n",
            "Installing collected packages: tld, justext, courlan, htmldate, trafilatura\n",
            "Successfully installed courlan-0.9.5 htmldate-1.6.0 justext-3.0.0 tld-0.13 trafilatura-1.6.3\n"
          ]
        }
      ],
      "source": [
        "!pip install feedparser\n",
        "!pip install dateparser\n",
        "!pip install trafilatura"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pygooglenews import GoogleNews\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import numpy as np\n",
        "import requests\n",
        "from requests.models import MissingSchema\n",
        "import spacy\n",
        "import trafilatura\n",
        "import time\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "MGSvPBh8rFF3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime, timedelta\n",
        "\n",
        "\n",
        "def beautifulsoup_extract_text_fallback(response_content):\n",
        "\n",
        "    '''\n",
        "    This is a fallback function, so that we can always return a value for text content.\n",
        "    Even for when both Trafilatura and BeautifulSoup are unable to extract the text from a\n",
        "    single URL.\n",
        "    '''\n",
        "\n",
        "    # Create the beautifulsoup object:\n",
        "    soup = BeautifulSoup(response_content, 'html.parser')\n",
        "\n",
        "    # Finding the text:\n",
        "    text = soup.find_all(text=True)\n",
        "\n",
        "    # Remove unwanted tag elements:\n",
        "    cleaned_text = ''\n",
        "    blacklist = [\n",
        "        '[document]',\n",
        "        'noscript',\n",
        "        'header',\n",
        "        'html',\n",
        "        'meta',\n",
        "        'head',\n",
        "        'input',\n",
        "        'script',\n",
        "        'style',]\n",
        "\n",
        "    # Then we will loop over every item in the extract text and make sure that the beautifulsoup4 tag\n",
        "    # is NOT in the blacklist\n",
        "    for item in text:\n",
        "        if item.parent.name not in blacklist:\n",
        "            cleaned_text += '{} '.format(item)\n",
        "\n",
        "    # Remove any tab separation and strip the text:\n",
        "    cleaned_text = cleaned_text.replace('\\t', '')\n",
        "    return cleaned_text.strip()\n",
        "\n",
        "\n",
        "def extract_text_from_single_web_page(url):\n",
        "\n",
        "    downloaded_url = trafilatura.fetch_url(url)\n",
        "    try:\n",
        "        a = trafilatura.extract(downloaded_url, with_metadata=True, include_comments = False, output_format = 'json',\n",
        "                            date_extraction_params={'extensive_search': True, 'original_date': True})\n",
        "    except AttributeError:\n",
        "        a = trafilatura.extract(downloaded_url, with_metadata=True, output_format = 'json',\n",
        "                            date_extraction_params={'extensive_search': True, 'original_date': True})\n",
        "    if a:\n",
        "        json_output = json.loads(a)\n",
        "        return json_output['text']\n",
        "    else:\n",
        "        try:\n",
        "            resp = requests.get(url)\n",
        "            # We will only extract the text from successful requests:\n",
        "            if resp.status_code == 200:\n",
        "                return beautifulsoup_extract_text_fallback(resp.content)\n",
        "            else:\n",
        "                # This line will handle for any failures in both the Trafilature and BeautifulSoup4 functions:\n",
        "                return np.nan\n",
        "        # Handling for any URLs that don't have the correct protocol\n",
        "        except MissingSchema:\n",
        "            return np.nan\n",
        "\n",
        "\n",
        "def get_all_dates_between(start_date_str, end_date_str):\n",
        "    start_date = datetime.strptime(start_date_str, '%Y-%m-%d')\n",
        "    end_date = datetime.strptime(end_date_str, '%Y-%m-%d')\n",
        "\n",
        "    current_date = start_date\n",
        "    date_list = []\n",
        "\n",
        "    while current_date <= end_date:\n",
        "        date_list.append(current_date.strftime('%Y-%m-%d'))\n",
        "        current_date += timedelta(days=1)\n",
        "\n",
        "    return date_list\n",
        "\n",
        "\n",
        "def get_news_between_dates(api, dates, topic):\n",
        "    news_list = []\n",
        "\n",
        "    for i in range(len(dates)-1):\n",
        "        from_date = dates[i]\n",
        "        to_date = dates[i+1]\n",
        "\n",
        "        result = api.search(topic, from_=from_date, to_=to_date)\n",
        "        news_list.append(result)\n",
        "\n",
        "    return news_list\n",
        "\n",
        "\n",
        "# Пример использования:\n",
        "gn = GoogleNews()\n",
        "\n",
        "topic = 'Microsoft'\n",
        "start_date_str = '2023-01-01'\n",
        "end_date_str = '2023-02-01'\n",
        "\n",
        "date_list = get_all_dates_between(start_date_str, end_date_str)\n",
        "news_result = get_news_between_dates(gn, date_list, topic)"
      ],
      "metadata": {
        "id": "BzsBRmGgpvQL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import requests\n",
        "# from requests.adapters import HTTPAdapter\n",
        "# from urllib3.util.retry import Retry\n",
        "\n",
        "# url = 'https://news.google.com/rss/articles/CBMiQWh0dHBzOi8vd3d3LnRlY2hyZXB1YmxpYy5jb20vYXJ0aWNsZS9taWNyb3NvZnQtaW52ZXN0bWVudC1vcGVuYWkv0gEA?oc=5'\n",
        "\n",
        "# headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36',}\n",
        "\n",
        "# r = requests.post(url, headers=headers)\n",
        "# r.raise_for_status()"
      ],
      "metadata": {
        "id": "eDzoHkLY4pth"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    with open('used-google-links.txt') as myfile:\n",
        "        used_links = myfile.readlines()\n",
        "        for i in range(len(used_links)):\n",
        "            used_links[i] = used_links[i].split()\n",
        "    used_links_df = pd.DataFrame(used_links, columns = ['id', 'id_short', 'google-link', 'link', 'date'])\n",
        "except:\n",
        "    used_links_df = pd.DataFrame(columns = ['id', 'id_short', 'google-link', 'link', 'date'])\n",
        "\n",
        "\n",
        "for day_idx in range(len(news_result)):\n",
        "    day_news = news_result[day_idx]['entries']\n",
        "\n",
        "    for news_idx in range(len(day_news)):\n",
        "        news = day_news[news_idx]\n",
        "        google_news_link = news['link']\n",
        "        news_date = time.strftime('%Y-%m-%d', news['published_parsed'])\n",
        "        title = news['title']\n",
        "        id = news['id']\n",
        "        id_short = id[:250]\n",
        "\n",
        "        if id not in used_links_df['id'].tolist() and \\\n",
        "           google_news_link not in used_links_df['google-link'].tolist():\n",
        "\n",
        "            headers = requests.utils.default_headers()\n",
        "\n",
        "            headers.update(\n",
        "                {\n",
        "                    'User-Agent': 'My User Agent 2.0',\n",
        "                }\n",
        "            )\n",
        "            news_url = requests.get(google_news_link, headers=headers).url\n",
        "            text = extract_text_from_single_web_page(url=news_url)\n",
        "\n",
        "            print(google_news_link)\n",
        "            print(news_url)\n",
        "            print(news_date)\n",
        "\n",
        "            with open(f\"/content/msft-texts/{id_short}.txt\", 'a') as myfile:\n",
        "                myfile.write(id)\n",
        "                myfile.write(\"\\n\")\n",
        "                myfile.write(title)\n",
        "                myfile.write(\"\\n\")\n",
        "                myfile.write(text)\n",
        "\n",
        "            with open(\"used-google-links.txt\", \"a\") as myfile:\n",
        "                myfile.write(id)\n",
        "                myfile.write(\" \")\n",
        "                myfile.write(id_short)\n",
        "                myfile.write(\" \")\n",
        "                myfile.write(google_news_link)\n",
        "                myfile.write(\" \")\n",
        "                myfile.write(news_url)\n",
        "                myfile.write(\" \")\n",
        "                myfile.write(news_date)\n",
        "                myfile.write(\"\\n\")\n",
        "\n",
        "            new_row = {\n",
        "                'id': id,\n",
        "                'id_short': id_short,\n",
        "                'google-link': google_news_link,\n",
        "                'link': news_url,\n",
        "                'date': news_date\n",
        "            }\n",
        "            used_links_df = pd.concat([used_links_df, pd.Series(new_row).to_frame().T], ignore_index = True)\n",
        "\n",
        "        # break\n",
        "    # break"
      ],
      "metadata": {
        "id": "XUjuTTqDlS9W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "475e5427-d133-4942-f13a-3345efac9cf7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:trafilatura.downloads:download error: https://news.google.com/rss/articles/CBMiQWh0dHBzOi8vd3d3LnRlY2hyZXB1YmxpYy5jb20vYXJ0aWNsZS9taWNyb3NvZnQtaW52ZXN0bWVudC1vcGVuYWkv0gEA?oc=5 HTTPSConnectionPool(host='news.google.com', port=443): Max retries exceeded with url: /rss/articles/CBMiQWh0dHBzOi8vd3d3LnRlY2hyZXB1YmxpYy5jb20vYXJ0aWNsZS9taWNyb3NvZnQtaW52ZXN0bWVudC1vcGVuYWkv0gEA?oc=5 (Caused by ResponseError('too many 503 error responses'))\n",
            "WARNING:trafilatura.core:discarding data for url: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://news.google.com/rss/articles/CBMiQWh0dHBzOi8vd3d3LnRlY2hyZXB1YmxpYy5jb20vYXJ0aWNsZS9taWNyb3NvZnQtaW52ZXN0bWVudC1vcGVuYWkv0gEA?oc=5\n",
            "https://news.google.com/rss/articles/CBMiQWh0dHBzOi8vd3d3LnRlY2hyZXB1YmxpYy5jb20vYXJ0aWNsZS9taWNyb3NvZnQtaW52ZXN0bWVudC1vcGVuYWkv0gEA?oc=5\n",
            "2023-01-24\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-1ae773ecfc89>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mmyfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mmyfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0mmyfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"used-google-links.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"a\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmyfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: write() argument must be str, not float"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojnhs1kW1Pty",
        "outputId": "12bb68a5-ab0b-4286-8a8c-e6faf3be9cad"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/msft-texts /content/gdrive/Shareddrives/gdrive/datasets/thesis\n",
        "# !cp used-google-links.txt /content/gdrive/Shareddrives/gdrive/datasets/thesis"
      ],
      "metadata": {
        "id": "_hdTCN9lK4C_"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GdvLLWKPLZAU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}